# SPT-LLMs: Safety-Performance Trade-off in Aligned Language Models

The SPT-LLMs repository implements the Safety-Performance Trade-off (SPT) Framework for aligned language models. It also provides a methodology for creating the HarmOR dataset of sensitive words, including extraction, generation, and moderation pipelines. Finally, the code for fine-tuning LLMs using the HarmOR dataset is included in this repository.

## Repository Structure

- **`spt_framework/`**  
  Core implementation of the SPT analysis and the SPT Framework.

- **`sensitive_words/`**  
  Dataset pipeline for HarmOR based on the sensitive words methodology. Includes extraction of sensitive words and generation of benign, harmful, prefix, and suffix variants.

- **`fine_tuning/`**  
  Scripts and datasets (with ground truths generated by GPT-4) for fine-tuning language models. Supports both LoRA and full-parameter fine-tuning.

- **`requirements.txt`**  
  Python dependencies (e.g., PyTorch, Hugging Face Transformers).

- **`LICENSE`**  
  MIT license text for the project.

## Requirements & Setup

To run the code, ensure you have Python 3.8 or higher:

1. Create and activate a virtual environment (or conda environment).  
2. Install dependencies:  
   ```bash
   pip install -r requirements.txt
   
## Usage

1. Run the SPT framework using the `eval_pipeline.py` script. Adjust the list of benchmarks, models, and defenses as needed.
2. Explore the HarmOR dataset in `harmor.csv` and experiment with the extraction, generation, and paraphrasing scripts.
3. To perform LoRA fine-tuning, run the `sft.py` script. For full-parameter fine-tuning, use the `sft_full.py` script.

## Disclaimer

This repository contains experimental software and is intended solely to provide additional background and reproducibility details for the corresponding master thesis. The code may be incomplete or subject to change and is not guaranteed to be production‐ready.

## Citation

If you use the corresponding master thesis in your work, please cite:

```bibtex
@mastersthesis{zadorin2025safety,
  author       = {Zadorin, Egor},
  title        = {Safety–Performance Trade‐off in Aligned Language Models},
  school       = {Technical University of Darmstadt},
  year         = {2025},
  type         = {Master's Thesis},
  address      = {Darmstadt, Germany}
}**


